{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2AighzIQleRV0GxsCV3iU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandreshraghuwanshi123456-jpg/ML_salary_predictor/blob/main/Copy_of_Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, MaxPooling3D, TimeDistributed, Dropout, Flatten, Dense\n",
        "import shutil\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "ZIP_NAME = \"jump.zip\"  # The file already uploaded\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
        "SEQUENCE_LENGTH = 20\n",
        "\n",
        "# --- 2. UNZIP DATA ---\n",
        "print(f\"üìÇ Looking for {ZIP_NAME}...\")\n",
        "extract_path = \"Training_Data\"\n",
        "\n",
        "if os.path.exists(extract_path):\n",
        "    shutil.rmtree(extract_path)\n",
        "\n",
        "if not os.path.exists(ZIP_NAME):\n",
        "    print(f\"‚ùå ERROR: Could not find '{ZIP_NAME}' in the Files sidebar.\")\n",
        "    print(\"üëâ Please drag and drop 'jump.zip' into the sidebar on the left.\")\n",
        "else:\n",
        "    with zipfile.ZipFile(ZIP_NAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"‚úÖ Unzip Complete!\")\n",
        "\n",
        "    # --- 3. AUTO-DETECT CLASSES ---\n",
        "    # We look inside the extracted folder to find class names automatically\n",
        "    CLASSES_LIST = []\n",
        "    for entry in os.listdir(extract_path):\n",
        "        full_path = os.path.join(extract_path, entry)\n",
        "        if os.path.isdir(full_path):\n",
        "            CLASSES_LIST.append(entry)\n",
        "\n",
        "    CLASSES_LIST.sort() # Ensure consistent order\n",
        "    print(f\"‚úÖ Found classes: {CLASSES_LIST}\")\n",
        "\n",
        "    if len(CLASSES_LIST) < 2:\n",
        "        print(\"‚ö†Ô∏è WARNING: Found fewer than 2 classes. Training might fail or be useless.\")\n",
        "\n",
        "    # --- 4. PREPARE DATA ---\n",
        "    def frames_extraction(video_path):\n",
        "        frames_list = []\n",
        "        video_reader = cv2.VideoCapture(video_path)\n",
        "        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "        for frame_counter in range(SEQUENCE_LENGTH):\n",
        "            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "            success, frame = video_reader.read()\n",
        "            if not success: break\n",
        "            resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "            normalized_frame = resized_frame / 255\n",
        "            frames_list.append(normalized_frame)\n",
        "        video_reader.release()\n",
        "        return frames_list\n",
        "\n",
        "    def create_dataset():\n",
        "        features, labels = [], []\n",
        "        print(\"‚öôÔ∏è Processing videos...\")\n",
        "\n",
        "        for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "            class_folder = os.path.join(extract_path, class_name)\n",
        "            if not os.path.isdir(class_folder): continue\n",
        "\n",
        "            print(f\"   -> Processing class: {class_name}\")\n",
        "            files = os.listdir(class_folder)\n",
        "\n",
        "            for file_name in files:\n",
        "                if file_name.lower().endswith(('.avi', '.mp4')):\n",
        "                    video_path = os.path.join(class_folder, file_name)\n",
        "                    frames = frames_extraction(video_path)\n",
        "                    if len(frames) == SEQUENCE_LENGTH:\n",
        "                        features.append(frames)\n",
        "                        labels.append(class_index)\n",
        "        return np.asarray(features), np.array(labels)\n",
        "\n",
        "    features, labels = create_dataset()\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(\"‚ùå ERROR: No videos found. Check folders inside zip.\")\n",
        "    else:\n",
        "        one_hot_encoded_labels = to_categorical(labels)\n",
        "        print(f\"‚úÖ Data Ready! {len(features)} videos found.\")\n",
        "\n",
        "        # --- 5. BUILD & TRAIN MODEL ---\n",
        "        print(\"üß† Building Neural Network...\")\n",
        "        model = Sequential()\n",
        "        model.add(ConvLSTM2D(filters=4, kernel_size=(3, 3), activation='tanh', data_format=\"channels_last\",\n",
        "                            return_sequences=True, input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "        model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "        model.add(TimeDistributed(Dropout(0.2)))\n",
        "        model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), activation='tanh', data_format=\"channels_last\", return_sequences=True))\n",
        "        model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(len(CLASSES_LIST), activation=\"softmax\"))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
        "\n",
        "        print(\"üèãÔ∏è Starting Training (40 Epochs)...\")\n",
        "        model.fit(features, one_hot_encoded_labels, epochs=40, batch_size=4, shuffle=True, validation_split=0.2)\n",
        "\n",
        "        # --- 6. SAVE ---\n",
        "        model_name = \"Final_Model.keras\"\n",
        "        model.save(model_name)\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"üéâ SUCCESS! The model is saved as '{model_name}'.\")\n",
        "        print(\"üëâ Right-click 'Final_Model.keras' in the Files sidebar -> Download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpPdMDifyWUY",
        "outputId": "b1f9f7f8-119f-40d8-8cdd-830b87224f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Looking for jump.zip...\n",
            "‚úÖ Unzip Complete!\n",
            "‚úÖ Found classes: ['jump', 'run', 'walk']\n",
            "‚öôÔ∏è Processing videos...\n",
            "   -> Processing class: jump\n",
            "   -> Processing class: run\n",
            "   -> Processing class: walk\n",
            "‚úÖ Data Ready! 29 videos found.\n",
            "üß† Building Neural Network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèãÔ∏è Starting Training (40 Epochs)...\n",
            "Epoch 1/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - accuracy: 0.4849 - loss: 2.0356 - val_accuracy: 0.0000e+00 - val_loss: 0.8541\n",
            "Epoch 2/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4695 - loss: 1.3012 - val_accuracy: 0.0000e+00 - val_loss: 2.7637\n",
            "Epoch 3/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.4474 - loss: 1.1827 - val_accuracy: 0.0000e+00 - val_loss: 1.1708\n",
            "Epoch 4/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5207 - loss: 1.0820 - val_accuracy: 0.0000e+00 - val_loss: 1.5525\n",
            "Epoch 5/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.4752 - loss: 1.0082 - val_accuracy: 0.0000e+00 - val_loss: 1.6321\n",
            "Epoch 6/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6068 - loss: 1.0061 - val_accuracy: 0.0000e+00 - val_loss: 1.4378\n",
            "Epoch 7/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3860 - loss: 1.0424 - val_accuracy: 0.0000e+00 - val_loss: 1.3606\n",
            "Epoch 8/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5572 - loss: 0.9616 - val_accuracy: 0.0000e+00 - val_loss: 1.7713\n",
            "Epoch 9/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4888 - loss: 0.9397 - val_accuracy: 0.0000e+00 - val_loss: 1.8250\n",
            "Epoch 10/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5959 - loss: 0.9537 - val_accuracy: 0.0000e+00 - val_loss: 1.6953\n",
            "Epoch 11/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6096 - loss: 0.8734 - val_accuracy: 0.0000e+00 - val_loss: 1.5826\n",
            "Epoch 12/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5186 - loss: 0.9156 - val_accuracy: 0.0000e+00 - val_loss: 1.6391\n",
            "Epoch 13/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5025 - loss: 0.8896 - val_accuracy: 0.0000e+00 - val_loss: 1.5980\n",
            "Epoch 14/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6834 - loss: 0.7284 - val_accuracy: 0.0000e+00 - val_loss: 1.7865\n",
            "Epoch 15/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6822 - loss: 0.7429 - val_accuracy: 0.0000e+00 - val_loss: 1.5969\n",
            "Epoch 16/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6852 - loss: 0.7374 - val_accuracy: 0.0000e+00 - val_loss: 1.2836\n",
            "Epoch 17/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6049 - loss: 0.7998 - val_accuracy: 0.0000e+00 - val_loss: 1.5860\n",
            "Epoch 18/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5930 - loss: 0.7604 - val_accuracy: 0.0000e+00 - val_loss: 1.7152\n",
            "Epoch 19/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6619 - loss: 0.6811 - val_accuracy: 0.0000e+00 - val_loss: 1.8232\n",
            "Epoch 20/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8069 - loss: 0.6179 - val_accuracy: 0.0000e+00 - val_loss: 1.5705\n",
            "Epoch 21/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6678 - loss: 0.7316 - val_accuracy: 0.1667 - val_loss: 1.3521\n",
            "Epoch 22/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6708 - loss: 0.6476 - val_accuracy: 0.0000e+00 - val_loss: 2.0906\n",
            "Epoch 23/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7717 - loss: 0.6128 - val_accuracy: 0.0000e+00 - val_loss: 1.9549\n",
            "Epoch 24/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7171 - loss: 0.4813 - val_accuracy: 0.0000e+00 - val_loss: 1.8020\n",
            "Epoch 25/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9715 - loss: 0.2595 - val_accuracy: 0.8333 - val_loss: 0.5303\n",
            "Epoch 26/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8533 - loss: 0.3964 - val_accuracy: 0.0000e+00 - val_loss: 4.1360\n",
            "Epoch 27/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7789 - loss: 0.5585 - val_accuracy: 0.0000e+00 - val_loss: 4.4578\n",
            "Epoch 28/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7682 - loss: 0.4366 - val_accuracy: 0.1667 - val_loss: 2.2144\n",
            "Epoch 29/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8052 - loss: 0.3531 - val_accuracy: 0.0000e+00 - val_loss: 2.0881\n",
            "Epoch 30/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8490 - loss: 0.4414 - val_accuracy: 0.0000e+00 - val_loss: 2.6360\n",
            "Epoch 31/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9680 - loss: 0.1814 - val_accuracy: 0.0000e+00 - val_loss: 3.2505\n",
            "Epoch 32/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9715 - loss: 0.1595 - val_accuracy: 0.1667 - val_loss: 3.6220\n",
            "Epoch 33/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0517 - val_accuracy: 0.0000e+00 - val_loss: 6.4203\n",
            "Epoch 34/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0755 - val_accuracy: 0.1667 - val_loss: 4.2039\n",
            "Epoch 35/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 0.3333 - val_loss: 1.0603\n",
            "Epoch 36/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9192 - loss: 0.2106 - val_accuracy: 0.0000e+00 - val_loss: 6.8094\n",
            "Epoch 37/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 0.1667 - val_loss: 4.6151\n",
            "Epoch 38/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0488 - val_accuracy: 0.1667 - val_loss: 5.3023\n",
            "Epoch 39/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0354 - val_accuracy: 0.0000e+00 - val_loss: 7.5365\n",
            "Epoch 40/40\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0412 - val_accuracy: 0.1667 - val_loss: 4.0980\n",
            "------------------------------\n",
            "üéâ SUCCESS! The model is saved as 'Final_Model.keras'.\n",
            "üëâ Right-click 'Final_Model.keras' in the Files sidebar -> Download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, MaxPooling3D, TimeDistributed, Dropout, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import shutil\n",
        "\n",
        "# ==========================================\n",
        "# PART 1: CONFIGURATION & TRAINING\n",
        "# ==========================================\n",
        "\n",
        "# ‚ö†Ô∏è Change this to match your uploaded file!\n",
        "ZIP_NAME = \"smart_subset.zip\"\n",
        "CLASSES_LIST = [\"Clapping\", \"Sitting\", \"Standing Still\", \"Walking\"]\n",
        "# (If using jump.zip, change to [\"jump\", \"run\", \"walk\"])\n",
        "\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
        "SEQUENCE_LENGTH = 20\n",
        "\n",
        "# 1. Unzip\n",
        "print(f\"üìÇ Unzipping {ZIP_NAME}...\")\n",
        "extract_path = \"Training_Data\"\n",
        "if os.path.exists(extract_path): shutil.rmtree(extract_path)\n",
        "\n",
        "if not os.path.exists(ZIP_NAME):\n",
        "    print(f\"‚ùå ERROR: Please upload '{ZIP_NAME}' to the Files sidebar first!\")\n",
        "else:\n",
        "    with zipfile.ZipFile(ZIP_NAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"‚úÖ Unzip Complete!\")\n",
        "\n",
        "    # 2. Process Data\n",
        "    def frames_extraction(video_path):\n",
        "        frames_list = []\n",
        "        video_reader = cv2.VideoCapture(video_path)\n",
        "        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "        for frame_counter in range(SEQUENCE_LENGTH):\n",
        "            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "            success, frame = video_reader.read()\n",
        "            if not success: break\n",
        "            resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "            normalized_frame = resized_frame / 255\n",
        "            frames_list.append(normalized_frame)\n",
        "        video_reader.release()\n",
        "        return frames_list\n",
        "\n",
        "    def create_dataset():\n",
        "        features, labels = [], []\n",
        "        print(\"üîç Scanning folders...\")\n",
        "        for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "            found = False\n",
        "            for root, dirs, files in os.walk(extract_path):\n",
        "                if os.path.basename(root).lower() == class_name.lower():\n",
        "                    print(f\"   -> Processing: {class_name}\")\n",
        "                    for file_name in files:\n",
        "                        if file_name.lower().endswith(('.avi', '.mp4')):\n",
        "                            video_path = os.path.join(root, file_name)\n",
        "                            frames = frames_extraction(video_path)\n",
        "                            if len(frames) == SEQUENCE_LENGTH:\n",
        "                                features.append(frames)\n",
        "                                labels.append(class_index)\n",
        "                                found = True\n",
        "            if not found: print(f\"‚ö†Ô∏è Warning: No videos found for {class_name}\")\n",
        "        return np.asarray(features), np.array(labels)\n",
        "\n",
        "    features, labels = create_dataset()\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(\"‚ùå CRITICAL ERROR: No data found. Check ZIP_NAME and CLASSES_LIST.\")\n",
        "    else:\n",
        "        one_hot_encoded_labels = to_categorical(labels)\n",
        "        print(f\"‚úÖ Data Ready! {len(features)} videos.\")\n",
        "\n",
        "        # 3. Build & Train\n",
        "        print(\"üß† Training Model...\")\n",
        "        model = Sequential()\n",
        "        model.add(ConvLSTM2D(filters=4, kernel_size=(3, 3), activation='tanh', data_format=\"channels_last\",\n",
        "                            return_sequences=True, input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "        model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "        model.add(TimeDistributed(Dropout(0.2)))\n",
        "        model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), activation='tanh', data_format=\"channels_last\", return_sequences=True))\n",
        "        model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(len(CLASSES_LIST), activation=\"softmax\"))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
        "        model.fit(features, one_hot_encoded_labels, epochs=30, batch_size=4, shuffle=True)\n",
        "        model.save(\"Colab_Brain.keras\")\n",
        "        print(\"üéâ TRAINING SUCCESS! Model saved.\")\n",
        "\n",
        "# ==========================================\n",
        "# PART 2: LIVE WEBCAM DEMO IN COLAB\n",
        "# ==========================================\n",
        "\n",
        "def start_webcam():\n",
        "  js = Javascript('''\n",
        "    async function createWebcam() {\n",
        "      const div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Create a canvas to grab the frame\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = 640;\n",
        "      canvas.height = 480;\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      // Label Element\n",
        "      const label = document.createElement('div');\n",
        "      label.textContent = \"Loading AI...\";\n",
        "      label.style.fontSize = \"24px\";\n",
        "      label.style.fontWeight = \"bold\";\n",
        "      label.style.color = \"red\";\n",
        "      div.appendChild(label);\n",
        "\n",
        "      return {video, canvas, label};\n",
        "    }\n",
        "\n",
        "    var webcam = null;\n",
        "\n",
        "    async function captureFrame() {\n",
        "      if (!webcam) {\n",
        "        webcam = await createWebcam();\n",
        "      }\n",
        "\n",
        "      // Draw video to canvas\n",
        "      const ctx = webcam.canvas.getContext('2d');\n",
        "      ctx.drawImage(webcam.video, 0, 0, 640, 480);\n",
        "\n",
        "      // Convert to base64\n",
        "      const result = webcam.canvas.toDataURL('image/jpeg', 0.5);\n",
        "      return result;\n",
        "    }\n",
        "\n",
        "    function updateLabel(text) {\n",
        "        if (webcam && webcam.label) {\n",
        "            webcam.label.textContent = text;\n",
        "            webcam.label.style.color = \"green\";\n",
        "        }\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "# START THE DEMO\n",
        "print(\"\\nüé• Starting Webcam Demo...\")\n",
        "start_webcam()\n",
        "\n",
        "frames_queue = []\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Get frame from JS\n",
        "        js_reply = eval_js('captureFrame()')\n",
        "        if not js_reply: break\n",
        "\n",
        "        frame = js_to_image(js_reply)\n",
        "\n",
        "        # Preprocess\n",
        "        resized = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        normalized = resized / 255.0\n",
        "        frames_queue.append(normalized)\n",
        "\n",
        "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
        "            input_data = np.expand_dims(np.array(frames_queue), axis=0)\n",
        "            prob = model.predict(input_data, verbose=0)[0]\n",
        "            pred_idx = np.argmax(prob)\n",
        "            action = CLASSES_LIST[pred_idx]\n",
        "            conf = prob[pred_idx]\n",
        "\n",
        "            # Send result back to JS to display on screen\n",
        "            status = f\"ACTION: {action.upper()} ({conf*100:.1f}%)\"\n",
        "            eval_js(f'updateLabel(\"{status}\")')\n",
        "\n",
        "            frames_queue = frames_queue[5:]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FrMXigLU1gT2",
        "outputId": "654f157a-7bad-426b-e2b5-0581af5d45b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Unzipping smart_subset.zip...\n",
            "‚ùå ERROR: Please upload 'smart_subset.zip' to the Files sidebar first!\n",
            "\n",
            "üé• Starting Webcam Demo...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function createWebcam() {\n",
              "      const div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Create a canvas to grab the frame\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = 640;\n",
              "      canvas.height = 480;\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      // Label Element\n",
              "      const label = document.createElement('div');\n",
              "      label.textContent = \"Loading AI...\";\n",
              "      label.style.fontSize = \"24px\";\n",
              "      label.style.fontWeight = \"bold\";\n",
              "      label.style.color = \"red\";\n",
              "      div.appendChild(label);\n",
              "\n",
              "      return {video, canvas, label};\n",
              "    }\n",
              "\n",
              "    var webcam = null;\n",
              "\n",
              "    async function captureFrame() {\n",
              "      if (!webcam) {\n",
              "        webcam = await createWebcam();\n",
              "      }\n",
              "\n",
              "      // Draw video to canvas\n",
              "      const ctx = webcam.canvas.getContext('2d');\n",
              "      ctx.drawImage(webcam.video, 0, 0, 640, 480);\n",
              "\n",
              "      // Convert to base64\n",
              "      const result = webcam.canvas.toDataURL('image/jpeg', 0.5);\n",
              "      return result;\n",
              "    }\n",
              "\n",
              "    function updateLabel(text) {\n",
              "        if (webcam && webcam.label) {\n",
              "            webcam.label.textContent = text;\n",
              "            webcam.label.style.color = \"green\";\n",
              "        }\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-v8s6CPz1l3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}